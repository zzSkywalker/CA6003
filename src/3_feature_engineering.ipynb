{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39305793-9dc7-4c91-a026-5c64e9ba0ecf",
   "metadata": {},
   "source": [
    "# PART III: Feature Engineering & Enrichment\n",
    "\n",
    "## 1. Executive Summary\n",
    "Following the data cleaning in Part II, this phase transforms the raw dataset into a machine-learning-ready format. Our approach goes beyond simple formatting; we integrate **domain knowledge** and **external data sources** to construct high-value features.\n",
    "\n",
    "**Key Highlights of Feature Engineering:**\n",
    "* **Geospatial Enrichment**: Integrated external government data (MRT stations & Malls) via API to quantify \"connectivity\" and \"lifestyle convenience\".\n",
    "* **Handling \"Cold Start\"**: Created `is_new_listing` flags to handle new properties with missing reviews, preserving their \"newness\" signal while enabling mathematical modeling.\n",
    "* **Text & Temporal Mining**: Extracted quantitative metrics from unstructured text (descriptions) and dates (host tenure).\n",
    "* **Dimensionality Expansion**: Applied One-Hot Encoding to categorical variables, expanding the dataset to **125+ numerical features** ready for direct modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a00d67d-38b6-42ec-a2e2-d0365089edd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded from ../data/listings_cleaned.csv. Shape: (3693, 78)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the cleaned dataset from Part II\n",
    "input_path = '../data/listings_cleaned.csv'\n",
    "df = pd.read_csv(input_path)\n",
    "print(f\"Data Loaded from {input_path}. Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704a303d-a2d1-4949-b6c6-78c0b8133de1",
   "metadata": {},
   "source": [
    "## 2. Advanced Preprocessing & Imputation Strategy\n",
    "\n",
    "### 2.1 Handling Systematic Missingness (Review Scores)\n",
    "A significant portion of listings (~50%) have missing `review_scores` not because of data errors, but because they are **new listings with zero reviews**. Dropping them would bias the model against new inventory.\n",
    "\n",
    "**Our Strategy:**\n",
    "1.  **Signal Preservation (`is_new_listing`)**: We created a binary feature `is_new_listing` (1 = New/No Reviews, 0 = Established). This allows the model to treat new listings differently from bad listings.\n",
    "2.  **Median Imputation**: We filled the missing scores with the **median value** of the dataset. This neutralizes the missing values mathematically, allowing algorithms to function without crashing, while the `is_new_listing` flag handles the actual \"newness\" effect.\n",
    "\n",
    "### 2.2 Final Cleanup\n",
    "We removed residual ID columns (e.g., `listing_url`, `host_thumbnail_url`) and text-heavy columns that were flagged as \"Governance/Privacy Risks\" or \"Noise\" in Part II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7499934c-af44-4c8e-9ac2-500dddc41f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Missing review scores imputed.\n"
     ]
    }
   ],
   "source": [
    "# 1. Convert Date Columns\n",
    "date_cols = ['host_since', 'first_review', 'last_review']\n",
    "for col in date_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# 2. Cleanup Residual Garbage Columns (if any remain from Part II)\n",
    "cols_to_drop = [\n",
    "    'listing_url', 'scrape_id', 'last_scraped', 'source', 'picture_url', \n",
    "    'host_url', 'host_thumbnail_url', 'host_picture_url', 'calendar_last_scraped',\n",
    "    'neighbourhood', 'host_neighbourhood', 'host_location', 'host_about', \n",
    "    'host_name', 'host_verifications', 'license', 'neighborhood_overview'\n",
    "]\n",
    "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors='ignore')\n",
    "\n",
    "# 3. Handle Systematic Missing Values (Review Scores)\n",
    "# Create a flag for listings with no reviews\n",
    "if 'review_scores_rating' in df.columns:\n",
    "    df['is_new_listing'] = df['review_scores_rating'].isnull().astype(int)\n",
    "\n",
    "# Impute missing scores with the median\n",
    "review_cols = [c for c in df.columns if 'review_scores' in c]\n",
    "for col in review_cols:\n",
    "    median_val = df[col].median()\n",
    "    df[col] = df[col].fillna(median_val)\n",
    "\n",
    "print(\"Preprocessing complete. Missing review scores imputed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d89aa-2a48-498a-925a-2478e88583e7",
   "metadata": {},
   "source": [
    "## 3. Feature Construction & Enrichment\n",
    "\n",
    "We engineered new features across three dimensions: **Text**, **Space**, and **Time**.\n",
    "\n",
    "### 3.1 Text Mining: Quantifying \"Luxury\" & \"Effort\"\n",
    "Raw text fields cannot be used directly in regression models. We extracted quantitative proxies:\n",
    "* **`amenities_count`**: Total number of amenities provided. (Hypothesis: More amenities = Higher Price).\n",
    "* **`has_pool`, `has_gym`, `has_ac`...**: Binary flags for high-value keywords extracted from the amenities list.\n",
    "* **`description_length` / `name_length`**: Character counts used as a proxy for **Host Effort** and listing detail.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85220e6d-71f9-42e2-869c-0c62be94e62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amenities features created.\n"
     ]
    }
   ],
   "source": [
    "def count_amenities(x):\n",
    "    try:\n",
    "        # Clean and count the items in the list string\n",
    "        return len(x.replace('[','').replace(']','').replace('\"','').split(','))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "if 'amenities' in df.columns:\n",
    "    # Feature 1: Count\n",
    "    df['amenities_count'] = df['amenities'].apply(count_amenities)\n",
    "    \n",
    "    # Feature 2: Key Amenities Flags\n",
    "    target_amenities = ['Air conditioning', 'Pool', 'Gym', 'Wifi', 'Kitchen', 'Washer']\n",
    "    for amenity in target_amenities:\n",
    "        col_name = f'has_{amenity.lower().replace(\" \", \"_\")}'\n",
    "        df[col_name] = df['amenities'].str.contains(amenity, case=False, regex=False).astype(int)\n",
    "    \n",
    "    # Drop original column\n",
    "    df = df.drop(columns=['amenities'])\n",
    "    \n",
    "print(\"Amenities features created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2127dff5-f062-4a94-8add-2ba93a190e0f",
   "metadata": {},
   "source": [
    "### 3.2 Geospatial and Temporal Features\n",
    "- **Distance to Center**: Calculated using the Haversine formula relative to Marina Bay Sands (City Center).\n",
    "- **Host Tenure**: Calculated as the number of years the host has been active.\n",
    "- **Name/Description Length**: Extracted as proxies for host effort and listing detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc817be2-72ef-4766-bcbc-c38233919b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geospatial and temporal features created.\n"
     ]
    }
   ],
   "source": [
    "# 1. Geospatial: Distance to Marina Bay Sands (1.2834, 103.8607)\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in km\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(dlambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "df['distance_to_center_km'] = haversine_distance(df['latitude'], df['longitude'], 1.2834, 103.8607)\n",
    "\n",
    "# 2. Temporal: Host Tenure (Years)\n",
    "ref_date = df['last_review'].max() # Reference date\n",
    "if 'host_since' in df.columns:\n",
    "    df['host_years'] = (ref_date - df['host_since']).dt.days / 365.0\n",
    "    df['host_years'] = df['host_years'].fillna(0)\n",
    "\n",
    "# 3. Text Length Features\n",
    "if 'name' in df.columns:\n",
    "    df['name_length'] = df['name'].fillna('').astype(str).apply(len)\n",
    "if 'description' in df.columns:\n",
    "    df['description_length'] = df['description'].fillna('').astype(str).apply(len)\n",
    "\n",
    "print(\"Geospatial and temporal features created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76b069-2320-4589-8992-15ed4bcc55d5",
   "metadata": {},
   "source": [
    "### 3.3 Geospatial Enrichment: MRT & Malls Proximity Analysis (New Feature)\n",
    "To capture the critical impact of location on property value, we extended the dataset with **external geospatial data** covering both transport and lifestyle amenities.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Data Acquisition**: We compiled geospatial data for all **Singapore MRT Stations** (Transport Connectivity) and **Major Shopping Malls** (Lifestyle Convenience).\n",
    "2.  **Geocoding**: Precise coordinates (Latitude/Longitude) were retrieved via the **Singapore Government OneMap API**.\n",
    "3.  **Feature Construction**: Using the Haversine formula, we calculated the **minimum distance** from each listing to the nearest MRT station and the nearest Shopping Mall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa6ad29-62dd-48c7-9834-5b1ed115355e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External Data Loaded:\n",
      "- MRT Stations: 213\n",
      "- Shopping Malls: 22\n",
      "Calculating distance to nearest MRT... (Step 1/2)\n",
      "Calculating distance to nearest Mall... (Step 2/2)\n",
      "✅ Geospatial Features created: 'dist_to_mrt_km', 'dist_to_mall_km'\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# NEW FEATURE: Proximity to Amenities (MRT & Malls)\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load External Data\n",
    "# Note: Ensure both 'mrt_stations.csv' and 'shopping_malls.csv' are in the '../data/' directory\n",
    "try:\n",
    "    mrt_path = '../data/mrt_stations.csv'\n",
    "    mall_path = '../data/shopping_malls.csv' # 假设你的商场数据文件名叫这个\n",
    "    \n",
    "    mrt_df = pd.read_csv(mrt_path)\n",
    "    mall_df = pd.read_csv(mall_path)\n",
    "    \n",
    "    print(f\"External Data Loaded:\")\n",
    "    print(f\"- MRT Stations: {len(mrt_df)}\")\n",
    "    print(f\"- Shopping Malls: {len(mall_df)}\")\n",
    "\n",
    "    # 2. Define Vectorized Haversine Function (Generic for any target points)\n",
    "    def get_nearest_distance(row, target_df):\n",
    "        R = 6371  # Earth radius in km\n",
    "        \n",
    "        # Convert degrees to radians\n",
    "        lat1, lon1 = np.radians(row['latitude']), np.radians(row['longitude'])\n",
    "        lat2 = np.radians(target_df['latitude'].values)\n",
    "        lon2 = np.radians(target_df['longitude'].values)\n",
    "        \n",
    "        # Haversine formula\n",
    "        dphi = lat2 - lat1\n",
    "        dlambda = lon2 - lon1\n",
    "        a = np.sin(dphi/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlambda/2)**2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "        distances = R * c\n",
    "        \n",
    "        # Return the minimum distance\n",
    "        return distances.min()\n",
    "\n",
    "    # 3. Apply Calculation for MRT\n",
    "    print(\"Calculating distance to nearest MRT... (Step 1/2)\")\n",
    "    df['dist_to_mrt_km'] = df.apply(lambda x: get_nearest_distance(x, mrt_df), axis=1)\n",
    "\n",
    "    # 4. Apply Calculation for Malls\n",
    "    print(\"Calculating distance to nearest Mall... (Step 2/2)\")\n",
    "    df['dist_to_mall_km'] = df.apply(lambda x: get_nearest_distance(x, mall_df), axis=1)\n",
    "    \n",
    "    print(\"✅ Geospatial Features created: 'dist_to_mrt_km', 'dist_to_mall_km'\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"⚠️ Warning: External data file not found. Details: {e}\")\n",
    "    # Create placeholders to prevent code failure\n",
    "    if 'dist_to_mrt_km' not in df.columns: df['dist_to_mrt_km'] = -1\n",
    "    if 'dist_to_mall_km' not in df.columns: df['dist_to_mall_km'] = -1\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error during proximity calculation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd1455-72dd-4a35-9b21-66a31047a4ae",
   "metadata": {},
   "source": [
    "## 4. Categorical Encoding & Output Generation\n",
    "\n",
    "### 4.1 Encoding Strategy\n",
    "* **Ordinal Encoding**: `host_response_time` was mapped to an ordered scale (4=Within an hour ... 1=Few days), reflecting the value of responsiveness.\n",
    "* **One-Hot Encoding**: Nominal variables like `room_type`, `neighbourhood_group`, and `property_type` were expanded into binary columns (0/1).\n",
    "    * *Note* : This expansion explains the increase in column count (from ~78 to ~125).\n",
    "* **Final Cleanup**: All remaining non-numeric columns are dropped to ensure the dataset is compatible with modeling algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a7a4fa0-5516-4937-a75e-31f66e408321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping remaining non-numeric columns: ['neighbourhood_cleansed']\n",
      "Feature Engineering Complete. Final Shape: (3693, 125)\n",
      "File saved to: ../data/listings_featured.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Ordinal Encoding for Host Response Time\n",
    "response_map = {\n",
    "    'within an hour': 4,\n",
    "    'within a few hours': 3,\n",
    "    'within a day': 2,\n",
    "    'a few days or more': 1\n",
    "}\n",
    "if 'host_response_time' in df.columns:\n",
    "    df['host_response_score'] = df['host_response_time'].map(response_map).fillna(0)\n",
    "\n",
    "# 2. Boolean Handling (ensure 1/0)\n",
    "bool_cols = ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', 'has_availability', 'instant_bookable']\n",
    "for col in bool_cols:\n",
    "    if col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "             df[col] = df[col].map({'t': 1, 'f': 0}).fillna(0)\n",
    "        else:\n",
    "             df[col] = df[col].astype(int)\n",
    "\n",
    "# 3. One-Hot Encoding\n",
    "categorical_cols = ['room_type', 'neighbourhood_group_cleansed', 'property_type']\n",
    "existing_cats = [c for c in categorical_cols if c in df.columns]\n",
    "df = pd.get_dummies(df, columns=existing_cats, drop_first=True)\n",
    "\n",
    "# 4. Final Drop of Processed/Text Columns\n",
    "cols_to_remove = ['name', 'description', 'host_response_time', 'host_since', 'first_review', 'last_review', 'bathrooms_text']\n",
    "df = df.drop(columns=[c for c in cols_to_remove if c in df.columns], errors='ignore')\n",
    "\n",
    "# 5. Sanity Check: Drop any remaining object columns\n",
    "non_numeric = df.select_dtypes(include=['object']).columns\n",
    "if len(non_numeric) > 0:\n",
    "    print(f\"Dropping remaining non-numeric columns: {list(non_numeric)}\")\n",
    "    df = df.drop(columns=non_numeric)\n",
    "\n",
    "# Export\n",
    "output_path = '../data/listings_featured.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Feature Engineering Complete. Final Shape: {df.shape}\")\n",
    "print(f\"File saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695fdea5-d732-41cc-b967-0dea4108234a",
   "metadata": {},
   "source": [
    "### 4.2 Final Deliverable (`listings_featured.csv`)\n",
    "The output file is a **strictly numerical matrix** optimized for Scikit-Learn/XGBoost.\n",
    "\n",
    "**Data Dictionary for New Features:**\n",
    "* `is_new_listing` (int): 1 if the listing has no reviews, 0 otherwise.\n",
    "* `dist_to_mrt_km` (float): Distance to the nearest MRT station in km.\n",
    "* `dist_to_mall_km` (float): Distance to the nearest Shopping Mall in km.\n",
    "* `amenities_count` (int): Total number of amenities.\n",
    "* `host_years` (float): Host tenure in years.\n",
    "* `host_response_score` (int): 1 (Slow) to 4 (Fast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6753f4-94d3-4aa0-9a57-7a7f5758f77c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
